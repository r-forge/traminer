% -*- TeX -*- -*- FR -*-
%input "C:\Documents and Settings\Matthias\Application Data\MiKTeX\2.9\bibtex\bib\stat.bib"
%input "C:\Documents and Settings\Matthias\Application Data\MiKTeX\2.9\bibtex\bib\bibliomat.bib"
%input "C:\Users\Matthias-Util\AppData\Local\MiKTeX\2.9\bibtex\bib\stat.bib"
%input "C:\Users\Matthias-Util\AppData\Local\MiKTeX\2.9\bibtex\bib\bibliomat.bib"


%\VignetteIndexEntry{Clustering}
%\VignetteIndexEntry{Weights}
%\VignetteIndexEntry{state sequences}
%\VignetteIndexEntry{Optimal matching}

\documentclass[article, nojss, a4paper, shortnames]{jss}

\usepackage[frenchb]{babel}
\usepackage{thumbpdf}
\usepackage{a4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{Matthias Studer\\  Institute for Demographic and Life Course Studies\\ University of Geneva}
\title{Le manuel de la librairie \pkg{WeightedCluster}\\{\large Un guide pratique pour la création de typologies de trajectoires en sciences sociales avec \proglang{R}}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Matthias Studer} %% comma-separated
\Plaintitle{Le manuel de la librairie WeightedCluster: un guide pratique pour la création de typologies de trajectoires en sciences sociales avec R} %% without formatting
\Shorttitle{Un guide pratique pour la création de typologies de trajectoires avec \proglang{R}} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
Ce manuel poursuit un double but: présenter la librairie \pkg{WeightedCluster} et proposer un guide pas à pas à la création de typologie de séquences pour les sciences sociales. Cette librairie permet notamment de représenter graphiquement les résultats d'une analyse en clusters hiérarchique, de regrouper les séquences identiques afin d'analyser un nombre de séquences plus important, de calculer un ensemble de mesure de qualité d'une partition, ainsi qu'un algorithme PAM optimisé prenant en compte les pondérations. La libraire offre également des procédures pour faciliter le choix d'une solution de clustering particulière et définir le nombre de groupes.

Outre les méthodes, nous discutons également de la construction de typologie de séquences en sciences sociales et des hypothèses sous-jacentes à cette démarche. Nous clarifions notamment la place que l'on devrait donner à la construction de typologie en analyse de séquences. Nous montrons ainsi que ces méthodes offrent un point de vue descriptif important sur les séquences en faisant ressortir des patterns récurrents. Toutefois, elles ne devraient pas être utilisées dans une optique confirmatoire, car elles peuvent conduire à des conclusions trompeuses.
}

\Keywords{Analyse de séquences, trajectoire, parcours de vie, optimal matching, distance, cluster, typologie, pondération, mesure de qualité d'une partition, \proglang{R}}
\Plainkeywords{Analyse de séquences, trajectoire, parcours de vie, optimal matching, distance, cluster, typologie, pondération, mesure de qualité d'une partition, R}

\Address{
  Matthias Studer\\
  Institute for Demographic and Life Course Studies\\
  University of Geneva\\
  CH-1211 Geneva 4, Switzerland\\
  E-mail: \email{matthias.studer@unige.ch}\\
  URL: \url{http://mephisto.unige.ch/weightedcluster/}
}

\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{afterpage}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage{lmodern}


\newcommand{\Com}[1]{\code{#1}}
\newcommand{\Comt}[1]{\code{#1}}
\newcommand{\File}[1]{\texttt{#1}}
\newcommand{\Filet}[1]{\texttt{#1}}
\newcommand{\Dataset}[1]{\code{#1}}
\newcommand{\Datasett}[1]{\code{#1}} % for dataset without index reference
\newcommand*\guil[1]{``#1''}

\newlength\TableWidth

\graphicspath{
	{./graphics/}
	}



\setcounter{topnumber}{4}          % \setcounter{topnumber}{2}
\def\topfraction{1}                % \def\topfraction{.7}
\setcounter{bottomnumber}{2}       % \setcounter{bottomnumber}{1}
\def\bottomfraction{1}             % \def\bottomfraction{.3}
\setcounter{totalnumber}{5}        % \setcounter{totalnumber}{3}
\def\textfraction{0}               % \def\textfraction{.2}
\def\floatpagefraction{1}          % \def\floatpagefraction{.5}


%% preliminary R commands
<<preliminary, echo=FALSE, results="hide", message=FALSE, warning=FALSE>>=
options(width=60, prompt="R> ", continue="     ", encoding="latin1", useFancyQuotes=FALSE, digits=3)
library(WeightedCluster)
library(TraMineR)

knit_hooks$set(crop = hook_pdfcrop)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before)  par(mar=c(2.1, 4.1, 4.1, 1.1))  # smaller margin on top and right
})
opts_knit$set(concordance=TRUE)
opts_chunk$set(message=FALSE, prompt=TRUE, echo=TRUE, comment=NA, crop=TRUE, small.mar=TRUE, fig.align="center", fig.path="graphics/WC-", out.width=".8\\linewidth")
knit_hooks$set(error = function(x, options) stop(x))
knit_hooks$set(warning = function(x, options) stop("Warnings: ", x))
@


\begin{document}

%\setkeys{Gin}{width=.9\linewidth}


\begin{center}
Pour citer ce document ou la librairie \pkg{WeightedCluster}, merci d'utiliser:\\~\\
\end{center}
Studer, Matthias (2012).\textit{\'Etude des inégalités de genre en début de carrière académique
à l'aide de méthodes innovatrices d'analyse de données séquentielles}, Chapitre: Le
manuel de la librairie WeightedCluster : Un guide pratique pour la création de typologies de trajectoires en sciences sociales avec R. Thèse SES 777, Faculté des sciences économiques et sociales, Université de Genève.

\section{Introduction}
Ce manuel poursuit un double but. Il présente les fonctionnalités offertes par la librairie \pkg{WeightedCluster} pour la construction et la validation de clustering de données pondérées dans \proglang{R}. En même temps, nous appliquons, tout au long de ce manuel, les méthodes présentées à l'analyse de séquences en sciences sociales, ce qui en fait également un guide pas à pas de la construction de typologie de séquences. Nous discutons également des implications et des hypothèses sociologiques sous-jacentes à ces analyses.

%La librairie \pkg{WeightedCluster} offre des fonctionnalités pour l'analyse en cluster à partir d'une matrice de dissimilarités. A l'heure actuelle, hormis la fonction \code{hclust} de la librairie \guil{base} que nous présentons ici, les méthodes disponibles dans \proglang{R} ne permettent pas de réalisés des analyses en cluster sur des données pondérées.

D'une manière générale, l'analyse en clusters a pour but de construire un regroupement d'un ensemble d'objets de telle manière que les groupes obtenus soient les plus homogènes possible et les plus différents possible les uns des autres. Il existe beaucoup de méthodes différentes pour réaliser cette opération dont la pertinence dépend notamment des objets analysés. Les méthodes présentées dans ce manuel et disponibles dans la librairie \pkg{WeightedCluster} se basent sur une mesure de dissimilarité entre les objets, ce qui permet de \textit{comparer} les objets en quantifiant leur similarité.

Nous présentons deux étapes de l'analyse en clusters basée sur des dissimilarités: les algorithmes de regroupement des objets avant d'aborder la mesure de la qualité des résultats obtenus. Cette dernière étape est essentielle puisque toutes les analyses en cluster présentées produisent des résultats que celui-ci soit pertinent ou non \citep{Levine2000}. Ces mesures fournissent également une aide précieuse pour choisir le meilleur regroupement parmi les solutions issues de différents algorithmes ou pour sélectionner le nombre de groupes optimal. Pour ce faire, nous utilisons ici les méthodes offertes par la librairie \pkg{WeightedCluster} telle qu'un algorithme PAM hautement optimisé ou encore le calcul et la visualisation de la qualité d'un ensemble de solutions de clustering.

La particularité de la librairie \pkg{WeightedCluster} est la prise en compte de la pondération des observations dans les deux phases de l'analyse décrites précédemment. Il y a aux moins deux cas de figure où l'utilisation de pondération se révèle indispensable. Premièrement, la pondération permet de regrouper les cas identiques, ce qui réduit considérablement la mémoire et le temps de calcul utilisé. La section~\ref{sec_aggregate} présente en détail les fonctionnalités proposées par la librairie \pkg{WeightedCluster} pour automatiser ce regroupement. Deuxièmement, les données d'enquête sont souvent pondérées pour  corriger les biais de représentativités. Dans ces cas de figure, il est essentiel d'utiliser les poids dans les procédures de clustering pour que les résultats ne soient pas biaisés. La librairie \pkg{WeightedCluster} offre des fonctions pour inclure les pondérations uniquement pour les analyses où il n'existe pas, à l'heure actuelle et à notre connaissance, d'autres librairies qui le font déjà. Si ceci devait être le cas, comme pour les procédures de clustering hiérarchiques, nous présentons les solutions existantes.

Comme mentionné précédemment, ce manuel se veut également un guide pas à pas pour la construction de typologie de trajectoires dans \proglang{R}. En tant que tel, ce manuel se destine à un public large, c'est pourquoi nous avons mis en annexe les parties plus techniques ou plus avancées\footnote{Toutefois, une connaissance des bases de l'analyse de séquences est supposée connue. Dans le cas contraire, une introduction à leurs mise en pratique avec \pkg{TraMineR} est disponible dans \citet{GabadinhoRitschardMullerStuder2011JSS}.}. Cette optique nous amènera également à discuter des implications et des hypothèses sociologiques sous-jacentes à l'analyse en clusters.

Dans ce manuel, nous illustrons les méthodes présentées à l'aide des données issues de l'étude de \citet{McVicarAnyadike2002JRSSa} qui sont disponibles dans \pkg{TraMineR} \citep{GabadinhoRitschardMullerStuder2011JSS} ce qui permettra au lecteur de reproduire l'ensemble des analyses présentées. Ces données décrivent sous forme de séquences de statuts mensuels la transition vers l'emploi de jeunes nord-irlandais ayant terminé l'école obligatoire. Le but original de l'étude était d'\guil{identify the `at-risk' young people at age 16 years and to characterize their post-school career trajectories}. Ce jeu de donnée est pondéré pour corriger les biais de représentativité.

La suite de ce manuel est organisée de la manière suivante. Nous commençons par présenter les enjeux théoriques de la construction de typologie de séquences en sciences sociales avant d'aborder les méthodes d'analyse en clusters à proprement parler. Nous reviendrons ensuite brièvement sur les différentes étapes de l'analyse en clusters avant de présenter plusieurs algorithmes de clustering disponibles dans \proglang{R} pour les données pondérées. Nous présentons ensuite plusieurs mesures de la qualité d'un clustering et les principales utilisations que l'on peut en faire. Finalement, nous discutons des enjeux de l'interprétation de l'analyse en clusters et des risques que cela comporte lorsque l'on analyse les liens entre types de trajectoires et des facteurs explicatifs, une pratique courante en sciences sociales.

%\begin{shadowblock}{\linewidth}
\section{Typologie de séquences en sciences sociales}
La création d'une typologie est la méthode la plus utilisée pour analyser des séquences~\citep{Hollister2009, AisenbreyFasang2010, AbbottTsay2000}. Cette procédure a pour but d'identifier les patterns récurrents dans les séquences ou, en d'autres termes, les successions d'états typiques par lesquelles passent les trajectoires. Les séquences individuelles se distinguent les une des autres par une multitude de petites différences. La construction d'une typologie des séquences a pour but de gommer ces petites différences afin d'identifier des types de trajectoires homogènes et distincts les uns des autres.

Cette analyse doit faire ressortir des patterns récurrents et/ou des \guil{séquences idéales-typiques} \citep{AbbottHrycak1990}. Ces patterns peuvent également s'interpréter comme des interdépendances entre différents moments de la trajectoire. La recherche de tels patterns est ainsi une question importante dans plusieurs problématiques de sciences sociales \citep{Abbott1995}. Elle permet notamment de mettre en lumière les contraintes légales, économiques ou sociales qui encadrent la construction des parcours individuels. Comme le notent \citet{AbbottHrycak1990}, si les séquences types peuvent résulter de contrainte que l'on redécouvre, ces séquences typiques peuvent également agir sur la réalité en servant de modèles aux acteurs qui anticipent leur propre futur. Ces différentes possibilités d'interprétations font de la création de typologie un outil puissant. Dans l'étude de \citet{McVicarAnyadike2002JRSSa} par exemple, une telle analyse devrait permettre d'identifier les successions d'états qui mènent à des situations `at-risk', c'est-à-dire marquées par un fort taux de chômage.

%Par la suite, les types de trajectoires identifiés sont souvent assimilés à des trajectoires types, c'est-à-dire des modèles de trajectoire, plus ou moins suivis par les individus.

Cette procédure de regroupement repose sur une \textit{simplification} des données. Elle offre ainsi un point de vue descriptif et exploratoire sur les trajectoires. En simplifiant l'information, on peut identifier les principales caractéristiques et les motifs des séquences. Toutefois, il existe un risque que cette \textit{simplification} soit \textit{abusive} et ne corresponde pas à une réalité des données. En d'autres termes, il est possible que les types identifiés ne soient pas clairement séparés les uns des autres ou qu'ils ne soient pas suffisamment homogènes. Ce risque a souvent été négligé dans la littérature. Comme le fait remarquer \citet{Levine2000} notamment, toutes les analyses en cluster produisent un résultat, que celui-ci soit pertinent ou non. Il est donc toujours possible d'interpréter les résultats et d'en faire une théorie qui se proclame souvent \guil{sans hypothèses préalables} ou \guil{issue des données}, alors que cette typologie peut également être le fruit d'un artifice statistique.

Selon \citet{Shalizi2009}, la pertinence d'une typologie dépend notamment de trois conditions. Premièrement, et c'est la plus importante, la typologie ne doit pas être dépendante de l'échantillonnage, ce qui signifie qu'elle doit être généralisable à d'autres observations. Deuxièmement, une typologie devrait s'étendre à d'autres propriétés. Finalement, la typologie obtenue devrait également être fondée par une théorie du domaine analysé. \citet{Shalizi2009} cite deux exemples pour illustrer ses propos. La classification des espèces animales, créées sur la base de caractéristiques physiques, s'applique également à d'autres caractéristiques telles que le chant d'un oiseau. Par contre, la classification des étoiles en constellations a permis de construire des théories sans fondements.

À notre connaissance, il n'existe pas de méthode pour \textit{attester} de la pertinence théorique d'une typologie. Toutefois, nous présentons ici plusieurs indices pour mesurer la qualité statistique d'une partition obtenue à l'aide d'une procédure de regroupement automatique. L'utilisation de telles mesures est à notre sens une étape essentielle pour valider les résultats et, d'une manière plus générale, rendre les résultats de l'analyse de séquences utilisant le clustering plus crédibles.

Maintenant que nous avons présenté les enjeux théoriques de l'analyse en clusters, nous passons à la pratique en présentant les différentes étapes de l'analyse en clusters.
%\end{shadowblock}



%The \pkg{WeightedCluster} library provides functions to cluster states sequences and more generally weighted data. These functionalities include finding duplicated cases, a weighted PAM algorithm, function computing cluster quality measures for a range of clustering solutions and miscellaneous functions to plot clustering solutions of state sequences.
%
%There are at least two reasons to use weighted data. First, some states sequences, or more generally some objects, may be equals. Regrouping those cases may improve algorithm efficiency. Most of all, regrouping duplicate case may also dramatically reduce the memory needed to store the dissimilarity matrix. Second, in the social sciences observations are often weighted to correct for response bias in surveys.


\section{Installation et chargement}

Pour utiliser la librairie \pkg{WeightedCluster}, il est nécessaire de l'installer et de la charger. Il suffit de l'installer une seule fois, mais elle devra être rechargée avec la commande \code{library} à chaque fois que \proglang{R} est lancé. Ces deux étapes sont réalisées de la manière suivante.


%% preliminary R commands
<<WCload, eval=FALSE>>=
install.packages("WeightedCluster", repos="http://R-Forge.R-project.org")
library(WeightedCluster)
@

\section{Étapes de l'analyse en clusters}

D'une manière générale, l'analyse en clusters se déroule en quatre étapes sur lesquelles nous reviendrons plus en détail. On commence par calculer les \textit{dissimilarités} entre séquences. On utilise ensuite ces dissimilarités pour regrouper les séquences similaires en types les plus homogènes possible et les plus différents possible les uns des autres. On teste généralement plusieurs algorithmes et nombres de groupes différents. On calcule ensuite pour chacun des regroupements obtenus des mesures de qualité. Ces mesures permettent de guider le choix d'une solution particulière et de la valider. Finalement, on interprète les résultats de l'analyse avant de mesurer éventuellement l'association entre le regroupement obtenu et d'autres variables d'intérêt.

Dans ce manuel, nous discuterons des trois dernières étapes de l'analyse. Nous n'offrons ici qu'une introduction au calcul de dissimilarités entre séquences. Rappelons notamment qu'une mesure de dissimilarité est une quantification de l'éloignement de deux séquences, ou d'une manière plus générale, de deux objets. Cette quantification permet ensuite de \textit{comparer} les séquences. Dans l'analyse en clusters par exemple, cette information est nécessaire pour regrouper les séquences les plus similaires. On utilise généralement une matrice de dissimilarités qui contient l'ensemble des dissimilarités deux à deux ou, en d'autres termes, la quantification de toutes les comparaisons possibles.

Le choix de la mesure de dissimilarité utilisée pour quantifier les différences entre séquences est une question importante, mais qui dépasse le cadre de ce manuel. Plusieurs articles adressent cette question \citep{AisenbreyFasang2010,Hollister2009,Lesnard-2010}. Ici, nous utilisons la distance d'optimal matching en utilisant les coûts utilisés dans l'article original de \citet{McVicarAnyadike2002JRSSa}.

Dans \proglang{R}, les distances entre séquences d'états peuvent être calculées à l'aide de la librairie \pkg{TraMineR} \citep{GabadinhoRitschardMullerStuder2011JSS}. Pour calculer ces distances, il faut premièrement créer un objet \guil{séquence d'états} à l'aide de la fonction \code{seqdef}. On calcule ensuite les distances avec la fonction \code{seqdist}. L'article \citet{GabadinhoRitschardMullerStuder2011JSS} présente en détail ces différentes étapes.

Ici, nous commençons par charger le fichier de donnée exemple avec la commande \code{data}. Nous construisons ensuite l'objet séquence en spécifiant les colonnes qui contiennent les données (17 à 86) ainsi que la variable de pondération des observations. La fonction \code{seqdist} calcule la matrice des distances entre séquences.

%% preliminary R commands
<<distcompute, tidy=FALSE>>=
data(mvad)
mvad.alphabet <- c("employment", "FE", "HE", "joblessness", "school",
    "training")
mvad.labels <- c("Employment", "Further Education", "Higher Education",
    "Joblessness", "School", "Training")
mvad.scodes <- c("EM", "FE", "HE", "JL", "SC", "TR")
mvadseq <- seqdef(mvad[, 17:86], alphabet = mvad.alphabet, states = mvad.scodes,
    labels = mvad.labels, weights=mvad$weight, xtstep=6)
## Defining the custom cost matrix
subm.custom <- matrix(
      c(0, 1, 1, 2, 1, 1,
        1, 0, 1, 2, 1, 2,
        1, 1, 0, 3, 1, 2,
        2, 2, 3, 0, 3, 1,
        1, 1, 1, 3, 0, 2,
        1, 2, 2, 1, 2, 0),
      nrow = 6, ncol = 6, byrow = TRUE)
## Computing the OM dissimilarities
mvaddist <- seqdist(mvadseq, method="OM", indel=1.5, sm=subm.custom)
@

% Plan
% Analyse de séquences et clustering
% Critiques et autres types d'analyses.
% Clustering de séquences en pratiques
%   % Procédures hiérarchiques
%   % Procédures partition
%   % Qualité du clustering
% Regroupement des séquences identiques
%
% Conclusion
%

La suite de ce manuel traite des trois étapes suivantes. Nous commençons par discuter des méthodes de clustering disponibles pour les données pondérées. Nous présenterons ensuite les mesures de la qualité d'un clustering offert par la librairie \pkg{WeightedCluster}. Ces mesures nous permettront de choisir une solution particulière et de mesurer sa validité. Finalement, nous discuterons des problèmes d'interprétation des résultats de l'analyse en clusters et du calcul des liens entre typologies et d'autres variables d'intérêts.


\section{Le clustering}

Il existe beaucoup d'algorithmes de clustering différents. Nous présentons ici des méthodes issues de deux logiques différentes: les méthodes de regroupement hiérarchique et celles de partitionnement en un nombre prédéfini de groupes. Nous concluons sur les interactions possibles entre ces types d'algorithmes.

\subsection{Clustering hiérarchique}

Nous présentons ici les procédures de regroupements hiérarchiques ascendantes qui fonctionnent ainsi. On part des observations, chacune d'entre elles étant considérée comme un groupe. À chaque itération, on regroupe les deux groupes (ou observations au départ) les plus proches, jusqu'à ce que toutes les observations ne forment plus qu'un seul groupe. Le schéma agglomératif, c'est-à-dire la succession des regroupements effectués, représente la procédure de clustering sous la forme d'un arbre que l'on appelle dendrogramme. Une fois le schéma agglomératif construit, on sélectionne le nombre de groupes. La partition retenue est obtenue en \guil{coupant} l'arbre de regroupement au niveau correspondant\footnote{Il existe également des procédures dites descendantes (ou divises) qui procèdent de la même manière, mais en sens inverse. Au lieu de démarrer des observations que l'on considère comme des groupes, la procédure divise à chaque pas un des groupes jusqu'à ce que chaque observation corresponde à un groupe. Un algorithme est disponible dans \proglang{R} avec la fonction \code{diana} de la librairie \code{cluster}. Tout ce que nous présentons ici est également compatible avec l'objet retourné par cette fonction.}.


Dans \proglang{R}, le schéma agglomératif (la succession des regroupements effectués) est créé avec la fonction \code{hclust}\footnote{d'autres possibilités existent.}. Cette fonction prend les paramètres suivants: la matrice de distances, la méthode (ici \guil{ward}, nous y reviendrons) ainsi que le vecteur de poids \code{members}.

%% preliminary R commands
<<hclustcompute>>=
wardCluster <- hclust(as.dist(mvaddist), method="ward", members=mvad$weight)
@

Une fois le schéma agglomératif créé, on peut visualiser les dernières étapes de ce schéma à l'aide de la librairie \pkg{WeightedCluster}. Pour ce faire, on procède en deux temps. On commence par construire un arbre de séquences à partir du schéma agglomératif avec la commande \code{as.seqtree}. Cette fonction prend les arguments suivant: la procédure de clustering (\code{wardCluster} dans notre cas), l'objet séquence (argument \code{seqdata}), la matrice de distance (argument \code{diss}) et le nombre maximum de regroupements à représenter (\code{ncluster}).

<<as.seqtreecompute>>=
wardTree <- as.seqtree(wardCluster, seqdata=mvadseq, diss=mvaddist, ncluster=6)
@

Une fois l'arbre construit, la fonction \code{seqtreedisplay} de la librairie \pkg{TraMineR} \citep{StuderRitschardGabadinhoMuller2011SMR} permet de le représenter graphiquement. L'option \code{showdepth=TRUE} affiche les niveaux des regroupements sur la droite du graphique.

<<seqtreedisplay, echo=FALSE, results="hide", eval=FALSE>>=
seqtreedisplay(wardTree, type="d", border=NA, filename="wardtree.png", showdepth=TRUE, showtree=FALSE)
@

<<seqtreedisplay-fake, eval=FALSE, echo=TRUE, results="hide">>=
seqtreedisplay(wardTree, type="d", border=NA, showdepth=TRUE)
@

%\begin{figure}[htb]
\begin{center}
  \includegraphics[width=.5\linewidth]{wardtree}
%\caption{Arbre des derniers regroupements, critère \guil{Ward}.}
%\label{fg_wardseqtree}
\end{center}
%\end{figure}
\afterpage{\clearpage}

Cette figure présente le résultat de cette procédure et permet de visualiser les logiques de regroupement des séquences. La première distinction, censée être la plus importante, sépare les jeunes irlandais qui se dirigent vers l'école supérieure des autres. Les distinctions qui se font lorsque l'on passe de quatre à cinq groupes nous permettent toutefois de mettre en lumière que ce groupe amalgame deux logiques: ceux qui vont vers l'école supérieure et ceux qui vont vers la \guil{Further Education}. Ce graphique fournit une aide importante pour identifier les distinctions pertinentes pour l'analyse. Ainsi, si l'on retient une solution en quatre groupes, on ne fera plus de distinction entre les deux logiques (\guil{École supérieure} et \guil{Further Education}). Par contre, la distinction en cinq groupes permet de l'identifier.

Cet exemple illustre la \textit{simplification} des données effectuées par les procédures de clustering. En ne retenant que quatre groupes, on amalgame deux logiques qui pourraient (ou non) être pertinentes pour l'analyse. Notons qu'en ne retenant que cinq groupes, on amalgame les distinctions qui se font à un niveau inférieur. On effectue ainsi toujours une simplification.

Pour obtenir un regroupement en un nombre de groupes particulier, on coupe l'arbre à un niveau donné. Ceci signifie que l'on garde tous les n\oe uds terminaux si l'arbre s'arrêtait au niveau présenter sur la droite. Dans \proglang{R}, on récupère cette information à l'aide de la fonction \code{cutree}. Par exemple, pour utiliser le regroupement en $4$ classes distinctes.


<<cutreecompute>>=
clust4 <- cutree(wardCluster, k=4)
@

On peut ensuite utiliser cette information dans d'autres analyses, par exemple, pour représenter les types obtenus à l'aide d'un chronogramme. Sans surprise, les graphiques obtenus correspondent aux n{\oe}uds terminaux de l'arbre au niveau quatre.

<<seqdplot-clust4, fig.width=7, fig.height=5>>=
seqdplot(mvadseq, group=clust4, border=NA)
@


Comme nous l'avons mentionné, la fonction \code{hclust} propose sept algorithmes hiérarchiques différents que l'on spécifie avec l'argument \code{method}. La librairie \pkg{fastcluster}  fournit une version optimisée de cette fonction \citep{Mullner2011}. Les algorithmes hiérarchiques  \code{diana} \citet[procédure descendante voir][]{Kaufman1990} et le bêta-flexible clustering avec la fonction \code{agnes} sont disponibles dans la librairie \pkg{cluster} \citep{RClusterPackage}\footnote{Si l'on utilise ces fonctions, il est nécessaire de spécifier l'argument \code{diss=TRUE}, pour que l'algorithme utilise la matrice de distance passée en argument.}. Le tableau~\ref{tb_hclustmethods} liste ces algorithmes en précisant le nom de la fonction à utiliser, la prise en compte des pondérations (l'entrée \guil{Indep} signifie que l'algorithme est insensible aux pondérations) et l'interprétation de la logique de clustering. Une présentation plus détaillée de ces algorithmes est disponible dans \citet{Mullner2011} et \citet{Kaufman1990}.

\begin{table}[htb]
\caption{Algorithmes de regroupements hiérarchiques.}
\label{tb_hclustmethods}
\begin{center}
{\scriptsize
\renewcommand\arraystretch{1.5}
\begin{tabular}{lllp{7.5cm}}
\toprule
Nom   & Fonction & Poids& Interprétation et notes. \\
\midrule
single & hclust & Indep &  Fusion des groupes avec les observations les plus proches.\\
complete & hclust & Indep  & Minimisation du diamètre de chaque nouveau groupe (très sensible aux données atypiques). \\
average (ou UPGMA) & hclust & Oui &  Moyenne des distances. \\
McQuitty (ou WPGMA) & hclust & Indep & Dépend des fusions précédentes. \\
centroid & hclust & Oui & Minimisation des distances entre médoïdes. \\
median & hclust & Indep & Dépend des fusions précédentes.  \\
ward & hclust & Oui & Minimisation de la variance résiduelle. \\
beta-flexible & agnes & Non & Pour une valeur de $\beta$ proche de $-0.25$, utiliser \code{par.method=0.625}.\\
\bottomrule
\end{tabular}%
}
\end{center}
\end{table}

Dans leur article, \citet{MilliganEtAl1987} reprennent les résultats de différentes simulations effectuées afin d'évaluer les performances de certains de ces algorithmes. Notons que ces simulations ont été réalisées avec des données numériques et la mesure de distance euclidienne. L'extension de ces résultats à d'autres types de distances est sujette à discussion. Ils reportent ainsi des résultats plutôt mauvais pour les méthodes \guil{single}, \guil{complete}, \guil{centroid} et \guil{median}. La méthode \guil{Ward} fait en général assez bien sauf en présence de données extrêmes qui biaisent les résultats. Ils reportent des résultats très variables pour la méthode \guil{average}. Finalement, la méthode \guil{beta-flexible} avec une valeur de bêta proche de $-0.25$ donne de bons résultats en présence de différentes formes d'erreur dans les données \citep{Milligan1989}. Les meilleurs résultats sont obtenus par l'algorithme \guil{flexible UPGMA} qui n'est pas disponible à l'heure actuelle dans \proglang{R} \citep{BelbinEtAl1992}.


Plusieurs critiques peuvent être adressées aux procédures hiérarchiques. Premièrement, et c'est la plus importante, la fusion de deux groupes se fait en maximisant un critère local. Ces procédures optimisent un critère local, c'est-à-dire qu'on estime localement la perte d'information due à un regroupement. Or, ces choix locaux peuvent mener à de grandes différences à des niveaux plus élevés et il n'est pas garanti qu'il soit les meilleurs d'un point de vue global. En d'autres termes, il arrive souvent qu'un choix bon au niveau local conduise à des résultats médiocres à un niveau de regroupement supérieur. Deuxièmement, les procédures ascendantes ne sont pas déterministes en particulier lorsque la mesure de distance ne prend que peu de valeurs différentes donnant lieu a des égalités entre lesquelles il faut trancher, ce qui peut notamment être le cas avec l'appariement optimal ou la distance de Hamming. Bien qu'une version particulière de cet algorithme produise généralement le même dendrogramme à chaque analyse\footnote{les algorithmes font généralement un choix qui dépend de l'ordre des observations, ce qui le rend reproductible pour autant que l'ordre soit inchangé.}, plusieurs versions de ce même algorithme peuvent conduire à des résultats divergents. De plus, ce cas de figure peut pénaliser la procédure qui n'a pas de critère pour effectuer un choix \citep{FernandezGomez2008}. Nous présentons à présent l'algorithme PAM qui a l'avantage de chercher à maximiser un critère global.


\subsection{Partitioning Around Medoids}


L'algorithme PAM pour \guil{Partitioning Around Medoids} suit une autre logique que les algorithmes hiérarchiques~\citep{Kaufman1990}. Il vise à obtenir la meilleure partition d'un ensemble de données en un nombre $k$ prédéfini de groupes. Par rapport aux autres algorithmes présentés, cet algorithme a l'avantage de maximiser un critère global et non uniquement un critère local.

Le but de l'algorithme est d'identifier les $k$ meilleurs représentants de groupes, appelés médoïdes. Plus précisément, un médoïde correspond à l'observation d'un groupe ayant la plus petite somme pondérée des distances aux autres observations de ce groupe. Cet algorithme cherche ainsi à minimiser la somme pondérée des distances au médoïde.

Brièvement, on peut décrire le fonctionnement de cet algorithme en deux phases. Dans un premier temps, on initialise l'algorithme en cherchant les observations qui diminuent le plus la somme pondérée des distances aux médoïdes existants, en choisissant le médoïde de l'ensemble des données au départ. Une fois la solution initiale construite, la deuxième phase de l'algorithme, appelée \guil{échange}, commence. Pour chaque observation, on calcule le gain potentiel si l'on remplaçait l'un des médoïdes existants par cette observation. Le gain est calculé au niveau global et en fonction des distances pondérées aux médoïdes les plus proches. On remplace ensuite le médoïde par l'observation qui conduit au plus grand gain possible. On répète ces opérations jusqu'à ce qu'il ne soit plus possible d'améliorer la solution courante.

L'algorithme est disponible dans la librairie R \pkg{cluster} \citep{RClusterPackage, StruyfHubertRousseeuw1997}, mais il ne permet pas d'utiliser des données pondérées. Basée en partie sur le code disponible dans la librairie \pkg{cluster}, la fonction \code{wcKMedoids} de librairie \pkg{WeightedCluster} prend en compte les pondérations et implémente également les optimisations proposées par \citet{Reynolds2006}, ce qui la rend plus rapide. Cette fonction consomme également deux fois moins de mémoire ce qui la rend adéquate pour analyser de très grands ensembles de données. L'annexe~\ref{annexe_optim} présente plus en détail les gains en temps de calcul.

<<wcKMedoids-compute4>>=
pamclust4 <- wcKMedoids(mvaddist, k=4, weights=mvad$weight)
@

L'élément \code{clustering} contient l'appartenance aux clusters de chaque observation. On peut, par exemple, utiliser cette information pour représenter les séquences à l'aide d'un chronogramme.

<<pamclust4-plot, fig.width=7, fig.height=5>>=
seqdplot(mvadseq, group=pamclust4$clustering, border=NA)
@

Le numéro assigné au groupe correspond à l'index du médoïde de ce groupe. On peut ainsi récupérer les médoïdes en utilisant la commande \code{unique(pamclust4\$clustering)}. La commande suivante utilise cette possibilité pour afficher les séquences médoïdes de chaque groupe.

<<wcKMedoids-print4>>=
print(mvadseq[unique(pamclust4$clustering), ], format="SPS")
@

L'algorithme PAM a plusieurs désavantages. Le premier est de créer des groupes \guil{sphériques}\footnote{Ce qui est également le cas du critère de \guil{Ward} dans les procédures hiérarchiques.} centrés autour de leur médoïde, ce qui ne correspond pas nécessairement à la réalité des données. Deuxièmement, il est nécessaire de spécifier le nombre $k$ de groupes au préalable. En testant plusieurs tailles $k$ de partition, on n'est pas assurés que les types obtenus s'emboîtent comme dans le cas des procédures hiérarchiques et le temps de calcul peut être conséquent. Finalement, l'algorithme est dépendant du choix des médoïdes de départs qui n'est pas toujours effectué de manière optimale.

\section{Combiner les algorithmes}

Les deux procédures de clustering peuvent être combinées, ce qui produit parfois de meilleurs résultats. Pour ce faire, on spécifie comme point de départ de la fonction \code{wcKMedoids} le clustering obtenu par la méthode hiérarchique (l'argument \code{initialclust=wardCluster}).

<<wcKMedoids-compute4-ward>>=
pamwardclust4 <- wcKMedoids(mvaddist, k=4, weights=mvad$weight, initialclust=wardCluster)
@

Ceci aboutit ici à une solution légèrement différente, mais avec une meilleure qualité.

<<pamwardclust4-plot-ward, fig.width=7, fig.height=5>>=
seqdplot(mvadseq, group=pamwardclust4$clustering, border=NA)
@


Nous avons présenté plusieurs types d'analyses en cluster qui nous ont conduits à des solutions différentes. Comment faire un choix entre ces solutions? Cette question est d'autant plus importante que nous aurions également pu sélectionner des nombres de groupes différents pour chaque procédure, ce qui nous amènerait à un grand nombre de possibilités. Les mesures de la qualité d'une partition que nous présentons maintenant aident à réaliser ce choix en offrant une base de comparaison de ces différentes solutions.


\section{Mesurer la qualité d'une partition}

Les mesures de la qualité d'une partition ont deux objectifs. Premièrement, certaines d'entre elles donnent une idée de la qualité statistique de la partition. Deuxièmement, ces mesures aident au choix de la meilleure partition d'un point de vue statistique. Elles sont ainsi d'une aide précieuse pour sélectionner le nombre de groupes ou le meilleur algorithme.


\subsection{Présentation des mesures}

La libraire \pkg{WeightedCluster} propose plusieurs mesures de la qualité d'une partition qui sont listées dans le tableau~\ref{tb_wcquality}. Le choix de ces mesures est largement inspiré par \citet{Hennig2010} que nous avons complété avec le \guil{C-index} qui figurait parmi les meilleurs indices selon \citet{MilliganCooper1985}. La présentation que nous faisons ici se centre sur les concepts. Le lecteur intéressé pourra toutefois se référer à l'annexe~\ref{annexe_clustqual} qui présente les détails mathématiques de ces mesures ainsi que les ajustements effectués pour prendre en compte la pondération des observations. Outre le nom des mesures de qualités, le tableau~\ref{tb_wcquality} présente leurs principales caractéristiques à l'aide des informations suivantes:
\begin{itemize}
  \item Abrv: abréviation utilisée dans la librairie \pkg{WeightedCluster}.
  \item Étendu: intervalle des valeurs possibles.
  \item Min/Max: Est-ce qu'une bonne partition minimise ou maximise cette mesure?
  \item Interprétation de la valeur.
\end{itemize}


\begin{table}[htb]
\caption{Mesures de la qualité d'un regroupement.}
\label{tb_wcquality}
\begin{center}
{\scriptsize
\renewcommand\arraystretch{1.5}
\begin{tabular}{p{3.5cm}lllp{6.5cm}}
\toprule
Nom   & Abrv. & Étendue & Min/Max & Interprétation\\
\midrule
Point Biserial Correlation & PBC & $[-1;1]$ & Max & Mesure de la capacité du clustering à reproduire les distances.\\
Hubert's Gamma & HG & $[-1;1]$ & Max & Mesure de la capacité du clustering à reproduire les distances (ordre de grandeur).\\
Hubert's Somers D & HGSD & $[-1;1]$ & Max & Mesure de la capacité du clustering à reproduire les distances (ordre de grandeur) avec prise en compte des égalités sur les distances.\\
Hubert's C & HC & $[0;1]$ & \textbf{Min} & Écart entre la partition obtenue et la meilleure partition qu'il serait théoriquement possible d'obtenir avec ce nombre de groupes et ces distances.\\
Average Silhouette Width & ASW & $[-1;1]$ & Max & Cohérence des assignations. Une cohérence élevée indique des distances inter-groupes élevées et une forte homogénéité intragroupe.\\
Average Silhouette Width (weighted) & ASWw & $[-1;1]$ & Max & Idem que précédant, si l'unité des poids n'a pas un sens explicite.\\
Calinski-Harabasz index & CH & $[0;+\infty[$ & Max & Pseudo F calculé à partir des distances.\\
Calinski-Harabasz index & CHsq& $[0;+\infty[$ & Max & Idem que précédant, mais en utilisant les distances \textit{au carré}.\\
Pseudo $R^2$ & R2 & $[0;1]$ & Max & Part de la dispersion expliquée par la solution de clustering (uniquement pour comparer des partitions avec nombre de groupes identiques).\\
Pseudo $R^2$ & R2sq & $[0;1]$ & Max & Idem que précédant, mais en utilisant les distances \textit{au carré}.\\
\bottomrule
\end{tabular}%
}
\end{center}
\end{table}

Les trois premières mesures, à savoir \guil{Point Biserial Correlation} \citep{MilliganCooper1985, Hennig2010}, \guil{Hubert's Gamma} et \guil{Hubert's Somers D} \citep{Hubert1985}, suivent la même logique. Elles mesurent la capacité d'une partition des données à reproduire la matrice des distances. Si la première mesure la capacité à reproduire la valeur exacte des distances, les deux suivantes se basent sur les concordances. Ceci implique que, selon ces deux derniers indices, une partition est bonne si les distances entre les groupes sont plus grandes que celles à l'intérieur des groupes. Techniquement, on mesure cette capacité en calculant l'association entre la matrice de distance et une deuxième mesure de distance qui prend la valeur $0$ pour les observations qui sont dans le même groupe et $1$ sinon. On utilise la corrélation de Pearson (\guil{Point Biserial Correlation}), Gamma de Goodman et Kruskal (\guil{Hubert's Gamma}) ou le D de Somers (\guil{Hubert's Somers D}).


L'indice \guil{Hubert's C} met en parallèle la partition obtenue et la meilleure partition que l'on aurait pu obtenir avec ce nombre de groupe et cette matrice de distance. Contrairement aux autres indices, une petite valeur indique une bonne partition des données.

Les indices de Calinski-Harabasz \citep{CalinskiHarabasz1974} se basent sur la statistique $F$ de l'analyse de variance. Cette mesure a donné de très bons résultats dans les simulations de \citet{MilliganCooper1985}. Toutefois, son extension à des données non numériques est sujette à discussion \citep{Hennig2010}. On peut discuter de sa pertinence si la mesure de distance est euclidienne (ou euclidienne au carré) auquel cas, cette mesure revient à utiliser la statistique $F$ sur les coordonnées que l'on peut associer aux observations, par exemple avec une analyse en coordonnées principales. Pour ce cas de figure, la librairie \pkg{WeightedCluster} fournit cette statistique en utilisant les carrés des distances lorsque la distance est euclidienne ou la distance elle-même lorsque la mesure est déjà une distance euclidienne au carré, comme la distance de Hamming par exemple.

Le \guil{R-square} calcule la part de la dispersion expliquée par une partition \citep{StuderRitschardGabadinhoMuller2011SMR}. Cette mesure n'est pertinente que pour comparer des partitions comportant le même nombre de groupe, car elle ne pénalise pas la complexité.

Finalement, la mesure \guil{Average Silhouette Width} proposée par \citet{Kaufman1990} est particulièrement intéressante. Elle se base sur la cohérence de l'assignation d'une observation à un groupe donné en mettant deux éléments en parallèle: la distance moyenne pondérée d'une observation aux autres membres de son groupe et la distance moyenne pondérée au groupe le plus proche. Une valeur de silhouette est calculée pour chaque observation. Si cette valeur est négative, l'observation est mal classée, car elle est plus proche d'un autre groupe que du sien. Au contraire, une valeur proche de 1 signifie que l'observation est proche de son groupe et loin de tous les autres. Généralement, on regarde plutôt la silhouette moyenne. Si celle-ci est faible, cela signifie que les groupes ne sont pas clairement séparés les uns des autres ou que l'homogénéité des groupes est faible. De manière intéressante, \citet{Kaufman1990} proposent des ordres de grandeur pour interpréter cette mesure que nous reproduisons dans le tableau~\ref{tab_kaufman_asw}.

\begin{table}[htb]
\caption{Ordres de grandeur pour interpréter la mesure $ASW$}
\label{tab_kaufman_asw}
\begin{center}
\begin{tabular}{ll}
    \toprule
        $ASW$ & Interprétation proposée\\
    \midrule
    0.71-1.00 & Structure forte identifiée.\\
    0.51-0.70 & Structure raisonnable identifiée.\\
    0.26-0.50 & La structure est faible et pourrait être artificielle.\\
              & Essayer d'autres algorithmes.\\
    $\leq 0.25$& Aucune structure.\\
    \bottomrule
\end{tabular}
\end{center}
\end{table}


La formulation originale de \citet{Kaufman1990} suppose que l'unité des pondérations corresponde à une observation, ce qui est le cas si les pondérations résultent d'agrégation (voir l'annexe~\ref{sec_aggregate}) ou si les données ne sont pas pondérées. Pour le cas où les pondérations visent à corriger la représentativité de données d'enquêtes (comme c'est le cas ici), nous proposons une variante de cette mesure appelée ``ASWw'' détaillée dans l'annexe~\ref{annexe_asw}. D'une manière générale, les résultats entre ces deux variantes sont très similaires, ``ASWw'' tendant à attribuer une qualité légèrement plus élevée.

Ces mesures sont calculées avec la fonction \code{wcClusterQuality}. La valeur retournée par la fonction est une liste qui comprend deux éléments. L'élément \code{stats} contient les valeurs des mesures de qualité.

<<wcClusterQuality-compute9, echo=2:3>>=
options(digits=2)
clustqual4 <- wcClusterQuality(mvaddist, clust4, weights=mvad$weight)
clustqual4$stats
options(digits=3)
@


Selon la mesure \code{ASWw=\Sexpr{round(clustqual4$stats["ASWw"], 2)}}, la solution en quatre groupes obtenue avec le critère de Ward pourrait être un artifice statistique, puisqu'elle est inférieure à 0.25.


L'élément \code{ASW} de l'objet \code{clustqual4} contient les deux variantes de la silhouette moyenne de chaque groupe pris séparément. Selon ces mesures, le groupe 3 est particulièrement mal défini puisque sa silhouette moyenne est négative.

<<wcClusterQuality-computeasw>>=
clustqual4$ASW
@

\subsection{Utiliser la silhouette pour représenter les clusters}

La silhouette peut être calculée séparément pour chaque séquence, ce qui permet d'identifier les séquences caractéristiques d'un regroupement (silhouette proche de un). La fonction \code{wcSilhouetteObs} calcule ces valeurs. Dans l'exemple ci-dessous, nous utilisons les silhouettes (avec la variante \code{measure="ASWw"}) pour ordonner les séquences dans des index-plots.

<<silhouette-indexplot, echo=2:3, dev="png", dpi=600, fig.width=7, fig.height=5>>=
par(mar=c(2.1, 4.1, 4.1, 1.1))
sil <- wcSilhouetteObs(mvaddist, clust4, weights=mvad$weight, measure="ASWw")
seqIplot(mvadseq, group=clust4, sortv=sil)
@

Les séquences les plus caractéristiques de chaque cluster sont représentées en haut de chaque graphique. Selon la définition de la silhouette, les séquences que nous appelons \guil{caractéristiques} sont celles qui sont proches du centre de leur groupe tout en étant éloignées du groupe le plus proche. Dans le groupe 1 par exemple, la séquence caractéristique est d'accéder à l'emploi après deux ans d'apprentissage. Au contraire, les séquences au bas de chaque graphique sont mal représentées et/ou mal assignées. Ainsi, dans le groupe 3 par exemple, les séquences \guil{école -- apprentissage -- emploi} sont plus proches d'un autre groupe (le 1 vraisemblablement) que de son propre groupe.


\subsection{Choix d'une partition}

Les mesures de la qualité d'une partition facilitent le choix de la meilleure partition parmi un ensemble de possibilités. On peut ainsi les utiliser pour identifier l'algorithme qui donne les meilleurs résultats. La fonction \code{wcKmedoids} utilisée pour PAM calcule directement ces valeurs qui sont stockées dans les éléments \code{stats} et \code{ASW} comme précédemment. Le code suivant affiche les mesures de qualités pour la partition identifiée à l'aide de PAM. Cette partition semble ainsi meilleure que celle obtenue avec Ward.

<<wcClusterQuality-compute91-option, echo=FALSE, results="hide">>=
options(digits=1)
@

<<wcClusterQuality-compute9-clustqual4pam>>=
pamclust4$stats
@

<<wcClusterQuality-compute91-option-end, echo=FALSE, results="hide">>=
options(digits=3)
@

Ces mesures permettent également de comparer des partitions avec un nombre de groupes différents. Seul le pseudo $R^2$ ne devrait pas être utilisé dans ce but, car il ne pénalise pas pour la complexité. Le calcul de la qualité de toutes ces différentes possibilités s'avère vite laborieux. La fonction \code{as.clustrange} de la librairie \pkg{WeightedCluster} calcule automatiquement ces valeurs pour un ensemble de nombres de groupes issus d'une même procédure de regroupement hiérarchique (\code{wardCluster} dans notre exemple). Cette fonction requiert les arguments suivants: la matrice des dissimilarités (\code{diss}), les pondérations (optionnel, argument \code{weights}) ainsi que le nombre maximum de cluster que l'on entend conserver (\code{ncluster}). Dans l'exemple suivant, nous estimons la qualité de clustering pour les regroupements en $2, 3, \ldots, \mbox{ncluster}=20$ groupes.

<<wcRange-compute9>>=
wardRange <- as.clustrange(wardCluster, diss=mvaddist, weights=mvad$weight, ncluster=20)
summary(wardRange, max.rank=2)
@

La fonction \code{summary} présente le meilleur nombre de groupes selon chaque mesure de qualité ainsi que la valeur de ces statistiques. L'argument \code{max.rank} spécifie le nombre de partitions à afficher. Selon la mesure \guil{Point Biserial Correlation} (PBC), la partition en six groupes est la meilleure des partitions, alors que pour l'indice \guil{ASW} une solution en deux groupes parait préférable. La valeur maximale identifiée pour ce dernier indice indique qu'il s'agit peut-être d'artifices statistiques (voir tableau~\ref{tab_kaufman_asw}). Rappelons que le pseudo-$R^2$ et sa version basée sur les distances élevées aux carrés donneront toujours un maximum pour le nombre de groupes le plus élevé, car ces statistiques ne peuvent pas diminuer.

La présentation offerte par \code{summary} est utile pour comparer les résultats de deux procédures (voir ci-dessous). Toutefois, elle ne présente que deux ou trois solutions et il est souvent utile d'observer l'évolution de ces mesures pour identifier les points de décrochages et les partitions qui offrent le meilleur compromis entre plusieurs mesures. La fonction \code{plot} associée à l'objet retourné par \code{as.clustrange} représente graphiquement cette évolution. Au besoin, l'argument \code{stat} spécifie la liste des mesures à afficher (\code{"all"} les affiches toutes).

<<wcRange-plot, fig.width=8, fig.height=3.5>>=
plot(wardRange, stat=c("ASWw", "HG", "PBC", "HC"))
@

La solution en six groupes est ici un maximum local pour les mesures \guil{HC}, \guil{PBC} et \guil{HG}, ce qui en fait une bonne si l'on souhaite garder un nombre de groupe restreint. La lecture du graphique est parfois un peu difficile, car les valeurs moyennes de chaque mesure diffèrent. Pour pallier ce problème, l'argument \code{norm="zscore"} standardise les valeurs, ce qui permet de mieux identifier les maximums et minimums\footnote{on peut utiliser \code{norm="zscoremed"} pour une standardisation plus robuste basée sur la médiane.}. La figure ci-dessous facilite l'identification des partitions qui donnent de bons résultats. Les solutions en six et en 17 groupes paraissent bonnes dans notre cas de figure.

<<wcRange-plot2, fig.width=8, fig.height=3.5>>=
plot(wardRange, stat=c("ASWw", "HG", "PBC", "HC"), norm="zscore")
@

L'objet retourné par la fonction \code{as.clustrange} contient également un \code{data.frame} contenant toutes les partitions testées dans l'élément \code{clustering}. On peut ainsi utiliser directement \code{as.clustrange} plutôt que \code{cutree}. La solution en six groupes peut être affichée de la manière suivante.

<<wardClust6-plot, fig.width=7, fig.height=6>>=
seqdplot(mvadseq, group=wardRange$clustering$cluster6, border=NA)
@

La fonction \code{as.clustrange} accepte divers types d'arguments, dont l'ensemble des procédures de clustering disponibles dans la librairie \pkg{cluster}, même si ces dernières n'acceptent pas de pondération. Toutefois, il n'est pas possible de l'utiliser directement avec les algorithmes non hiérarchiques tels que PAM. Pour ce faire, on utilise la fonction \code{wcKMedRange} qui calcule automatiquement les partitions pour une série de valeurs de $k$ (nombre de groupes). L'objet retourné est le même que celui présenté précédemment pour les procédures de regroupements hiérarchiques et l'on peut donc utilisé les mêmes présentations que précédemment. Cette fonction prend en paramètre une matrice de distance, \code{kvals} un vecteur contenant le nombre de groupes des différentes partitions à créer, et un vecteur de poids. Les arguments supplémentaires sont passés à \code{wcKMedoids}.

<<wcKMedRange-compute, echo=TRUE>>=
pamRange <- wcKMedRange(mvaddist, kvals=2:20, weights=mvad$weight)
summary(pamRange, max.rank=2)
@

La présentation offerte par la fonction \code{summary} est particulièrement utile pour comparer les solutions de différentes procédures de clustering. On peut ainsi remarquer que les résultats de PAM sont généralement plus performants. Outre la solution en deux groupes, peut-être un peu trop simplificatrice, celle en quatre groupes parait ici adaptée. Remarquons tout de même que ces partitions pourraient toujours être le fruit d'un artifice statistique puisque l'\code{ASW} est inférieur à $0.5$.

Les différents outils que nous avons présentés nous ont permis de construire une typologie des séquences. Pour ce faire, nous avons testé divers algorithmes et nombres de groupes avant de retenir une partition en quatre groupes créer à l'aide de la méthode PAM. D'une manière générale, nous suggérons aux lecteurs de tester un plus grand nombre d'algorithmes que ce que nous avons fait ici. \citet{lesnard2006} suggère par exemple d'utiliser la méthode \guil{average} ou la méthode \guil{flexible} (voir tableau~\ref{tb_hclustmethods}). Les outils présentés permettent de faire ces comparaisons.

\subsection{Nommer les clusters}

Une fois la typologie créée, il est d'usage de nommer les types obtenus afin de rendre leur interprétation plus facile. La fonction \code{factor} permet de créer une variable catégorielle. Pour ce faire, on spécifie la variable des types (ici \code{pamclust4\$clustering}), les valeurs que prend cette variable avec l'argument \code{levels} (on peut retrouver ces valeurs dans les graphiques précédents), ainsi que les labels que l'on souhaite attribuer à chacun de ces types avec l'argument \code{labels}. Les \code{labels} doivent être spécifiés dans le même ordre que l'argument \code{levels} de sorte que la première entrée de \code{levels} (ici 66) corresponde à la première entrée de \code{labels} (ici \guil{Apprentissage-Emploi}).


<<cluster-naming, echo=TRUE>>=
mvad$pam4 <- factor(pamclust4$clustering, levels=c(66, 467, 607, 641), labels=c("Appr.-Empl.", "Ecole-Empl.", "Ecole sup.", "Sans empl."))
@

La fonction \code{seqclustname} permet de nommer automatiquement les groupes en utilisant leur médoïde. On doit spécifier l'objet séquence (agument \code{seqdata}), le clustering (argument \code{group}), la matrice de distance (argument \code{diss}). Si \code{weighted=TRUE} (par défaut), les poids de l'objet \code{seqdata} sont utilisés pour trouver les médoïdes. Finalement, l'option \code{perc=TRUE} ajoute le pourcentage d'observation dans chaque groupe aux labels.

<<auto-cluster-naming, echo=TRUE>>=
mvad$pam4.auto <- seqclustname(mvadseq, pamclust4$clustering, mvaddist)
table( mvad$pam4.auto, mvad$pam4)
@


Une fois les clusters nommés, la typologie est construite. Avant de conclure, nous revenons plus en détail sur la question de la simplification induite par l'analyse en clusters et les biais que celle-ci peut introduire dans l'analyse. Nous illustrons cette question en présentant le calcul des liens entre typologie et facteurs explicatifs et en montrant comment ceci peut biaiser les résultats.



\section{Mettre en lien trajectoires-types et facteurs explicatifs}

Beaucoup de questions de recherche des sciences sociales mettent en lien des trajectoires (ou des séquences) avec des facteurs explicatifs. Ainsi, on peut se demander si les trajectoires professionnelles des hommes diffèrent significativement de celles des femmes. De manière similaire, \citet{WidmerEtAll2003} cherchent à mettre en évidence l'apparition de nouvelles trajectoires de construction de la vie familiale. Pour ce faire, on met généralement en lien une typologie des séquences familiales avec la cohorte des individus à l'aide de test du khi-carré ou de régression logistique \citep{AbbottTsay2000}. Dans cette section, nous présentons cette technique ainsi que les dangers qu'elle comporte pour l'analyse.

Supposons que l'on cherche à mesurer les liens entre la variable \code{test} que nous avons créée pour l'occasion\footnote{Le détail de la création de cette variable est donné dans l'annexe~\ref{annexe_vartest}.} et nos trajectoires. Cette variable prend deux modalités, \guil{test} et \guil{non-test}, et regroupe les séquences de la manière suivante.

<<mds-compute, echo=FALSE, results="hide", message=FALSE>>=
library(vegan)
library(isotone)
@
<<mds-compute2, echo=FALSE, results="hide">>=
worsq <- wcmdscale(mvaddist, w=mvad$weight, k=2)
@

<<mds-compute3, echo=FALSE, results="hide">>=
mvad$test <- rep(-1, nrow(mvad))
for(clust in unique(pamclust4$clustering)){
    cond <- pamclust4$clustering == clust
    values <- worsq[cond, 2]
    mvad$test[cond] <- as.integer(values > weighted.median(values, w=mvad$weight[cond]))
}
mvad$test <- factor(mvad$test, levels=0:1, labels=c("non-test", "test"))

@


<<testdplot, fig.width=10, fig.height=4.5>>=
seqdplot(mvadseq, group=mvad$test, border=NA)
@

Les individus \guil{non-test} semblent notamment avoir une plus forte probabilité de connaître une période sans emploi. Est-ce que cette différence est significative? On peut faire un test du khi-carré entre la variable test et pam4 (qui reprend nos types) de la manière suivante pour des données pondérées. On commence par construire un tableau croisé avec la fonction \code{xtabs}\footnote{Pour des données non pondérées, on peut utiliser la fonction \code{table}.}. Cette fonction prend en paramètre une formule, dont le côté gauche de l'équation fait référence aux pondérations et le côté droit liste les facteurs qui forment le tableau croisé. L'argument \code{data} spécifie où il faut chercher les variables qui apparaissent dans la formule. On calcule ensuite un test du khi-carré sur ce tableau.

<<testiplot-chisq>>=
tb <- xtabs(weight~test+pam4, data=mvad)
chisq.test(tb)
@

Le résultat est non significatif, ce qui signifie que selon cette procédure les trajectoires ne diffèrent pas selon la variable \code{test}. Quel est le problème? Est-ce vraiment le cas? Non, le problème vient de la simplification de l'analyse en clusters. En utilisant la typologie dans le test du khi-carré, on fait implicitement l'hypothèse que cette typologie est suffisante pour décrire la complexité des trajectoires, or ce n'est pas le cas ici. En effet, la variable test explique la variation des trajectoires \textit{au sein des types}. À titre d'exemple, le graphique suivant montre les différences de trajectoires selon la variable test pour les individus classés dans le type \guil{École-Emploi}. Les individus \guil{non-test} semblent ainsi avoir un risque plus élevé de connaître un épisode \guil{Sans emploi}. La variabilité au sein de ce type ne semble donc pas négligeable.

<<testdplot-EE, fig.width=10, fig.height=4.5>>=
EcoleEmploi <- mvad$pam4=="Ecole-Empl."
seqdplot(mvadseq[EcoleEmploi, ], group=mvad$test[EcoleEmploi], border=NA)
@

Revenons sur l'hypothèse sous-jacente dont nous avons parlé et tâchons de l'expliciter. En utilisant la typologie, on assigne à chaque séquence son type. On ignore ainsi l'écart entre la trajectoire réellement suivie par un individu et son type. Une des justifications possibles de cette opération serait de dire que les types obtenus correspondent aux modèles qui ont effectivement généré les trajectoires. Les écarts entre les trajectoires suivies et ces modèles (c'est-à-dire les types) seraient ainsi assimilables à une forme de terme d'erreur aléatoire ne contenant aucune information pertinente. Cette méthode revient donc à faire l'hypothèse que les trajectoires sont générées par des modèles établis et clairement distincts les uns des autres. De plus, nous faisons implicitement l'hypothèse que nous avons effectivement réussi à retrouver les vrais modèles à l'aide de l'analyse en clusters.

Parallèlement, on fait également une deuxième hypothèse, à savoir que les types obtenus sont également différents les uns des autres\footnote{Les cartes de Kohonen permettent de visualiser ces distances entre types de trajectoires \citep{RoussetGiret2007}.}. Or, ce n'est pas le cas. Les types \guil{Apprentissage-Emploi} et \guil{Ecole-Emploi} sont plus proches, car ils partagent une fin de trajectoires identiques. Quant à eux, les types \guil{école supérieure} et \guil{sans emploi} sont particulièrement éloignés.
%
%<<testplot-scatter, fig=TRUE, width=10, height=5.5>>=
%par(mfrow=c(1,2))
%plot(worsq, col=as.integer(mvad$pam4), pch=as.integer(mvad$pam4), main="Clustering", xlab="Dim 1", ylab="Dim 2")
%legend("bottomleft", legend=levels(mvad$pam4), fill=1:4)
%plot(worsq, col=(mvad$test+5), pch=(mvad$test+5), xlab="Dim 1", ylab="Dim 2")
%legend("bottomleft", legend=c("0", "1"), fill=5:6)
%@

Ces deux hypothèses sont des hypothèses fortes. On postule l'existence de modèles qui ont effectivement généré les trajectoires, ce qui est discutable d'un point de vue sociologique. Dans la lignée du paradigme des parcours de vie, on peut ainsi penser que les individus sont soumis à des influences et des contraintes diverses qui participent, chacune à leur manière, à la construction de la trajectoire (ou d'une partie de la trajectoire). On est bien loin de la recherche de modèles de trajectoires.

Encore une fois, ces hypothèses peuvent s'avérer pertinentes si les groupes obtenus sont très homogènes et très différents les uns des autres. Une situation où la silhouette moyenne devrait être relativement élevée ($ASW>0.7$ par exemple). Mais ce n'est pas le cas ici, puisque la silhouette moyenne est inférieure à $0.5$.

La solution à ce problème consiste à utiliser l'analyse de dispersion \citep{StuderRitschardGabadinhoMuller2011SMR}. Ce type d'analyse permet de mesurer la force du lien en fournissant un pseudo-$R^2$, c'est-à-dire la part de la variation expliquée par une variable, ainsi que la significativité de l'association. On s'affranchit ainsi de l'hypothèse des modèles de trajectoires en calculant directement le lien, sans clustering préalable. On trouvera dans \citet{StuderRitschardGabadinhoMuller2011SMR} une introduction à sa mise en pratique dans \proglang{R} ainsi qu'une présentation générale de la méthode. Nous n'offrons donc ici qu'un survol destiné à étayer notre propos.

Brièvement, un test bivarié de l'association entre les séquences et la variable \code{test} peut être calculé avec la fonction \code{dissassoc} disponible dans la librairie \pkg{TraMineR}. On lit les résultats qui nous intéressent ici sur la ligne Pseudo $R^2$. On remarque ainsi que la variable test permet d'expliquer 3.6\% de la variabilité des trajectoires et la $p$-valeur est largement significative.


<<testiplot-dissassoc>>=
set.seed(1)
dsa <- dissassoc(mvaddist, mvad$test, weights=mvad$weight, weight.permutation="diss", R=5000)
print(dsa$stat)
@

On peut inclure plusieurs variables dans l'analyse à l'aide d'un arbre de régression sur les séquences\footnote{Il est également possible d'analyser les effets conjoints avec une approche multifacteur.}. Pour ce faire, on utilise la fonction \code{seqtree}. Cette analyse met en evidence les variables les plus importantes ainsi que leurs interactions. Ici, la variable \code{gcse5eq} (bon résultats à la fin de l'école obligatoire) à l'effet le plus important. Pour ceux qui ont eu de bon résultats, avoir suivi une école obligatoire de type \code{Grammar} favorise l'accès à l'école supérieure. Par contre, pour ceux qui ont eu de mauvais résultats, c'est d'avoir un père au chômage qui est le plus significatif. On semble alors avoir plus de chance de se retrouver au sans emploi.

<<seqtree-link-covar, echo=FALSE, results="hide", eval=FALSE>>=
set.seed(1)
tree <- seqtree(mvadseq~gcse5eq+Grammar+funemp, data=mvad, diss=mvaddist, weight.permutation="diss")
seqtreedisplay(tree, type="d", border=NA, filename="seqtree.png", showtree=FALSE)
@

<<seqtree-link-covar-fake, eval=FALSE, echo=TRUE, results="hide">>=
tree <- seqtree(mvadseq~gcse5eq+Grammar+funemp, data=mvad, diss=mvaddist, weight.permutation="diss")
seqtreedisplay(tree, type="d", border=NA)
@

\begin{center}
  \includegraphics[width=.9\linewidth]{seqtree}
%\caption{Arbre des derniers regroupements, critère \guil{Ward}.}
%\label{fg_wardseqtree}
\end{center}
%\end{figure}
\afterpage{\clearpage}

L'analyse de dispersion conduit à un changement de paradigme et à s'affranchir du concept de modèle de trajectoire. Plutôt que de se baser sur la recherche de modèles, nous considérons que les trajectoires s'insèrent dans un contexte qui influence à sa manière la construction de la trajectoire. En d'autres termes, nous cherchons à comprendre dans quelle mesure la variabilité interindividuelle est expliquée par un contexte tout en rendant compte de la diversité des chemins empruntés. D'un point de vue conceptuel, les hypothèses sous-jacentes aux méthodes à l'analyse de dispersion sont très proches des principes mis en avant par le paradigme des parcours de vie. \citet{Elder1999} insiste ainsi sur la nécessité d'insérer les parcours dans des temps et des lieux (le contexte) tout en préservant la variabilité interindividuelle et l'internationalité des acteurs.

À notre sens, la construction de typologie de séquence est une méthode puissante qui a l'avantage d'offrir un point de vue descriptif sur les séquences en réduisant la complexité de l'analyse. Toutefois, son utilisation en lien avec des méthodes inférentielles doit être faite avec prudence, puisqu'elle peut conduire à des conclusions trompeuses, comme illustré avec la variable \code{test}.

\section{Conclusion}

Ce manuel poursuivait un double but: présenter la librairie \pkg{WeightedCluster} et proposer un guide pas à pas à la création de typologie de séquences pour les sciences sociales. Cette librairie permet notamment de représenter graphiquement les résultats d'une analyse en clusters hiérarchique, de regrouper les séquences identiques afin d'analyser un nombre de séquences plus important\footnote{Le détails de cette procédure est explicité dans l'annexe~\ref{sec_aggregate}.}, de calculer un ensemble de mesure de qualité d'une partition ainsi qu'une version optimisée de l'algorithme PAM prenant en compte les pondérations. La libraire offre également des procédures pour faciliter le choix d'une solution de clustering particulière et définir le nombre de groupes.


Outre les méthodes, nous avons également discuté de la construction de typologie de séquences en sciences sociales. Nous avons ainsi argumenté que ces méthodes offrent un point de vue descriptif important sur les séquences. L'analyse en clusters permet de faire ressortir des patterns récurrents et/ou des \guil{séquences idéales-typiques} \citep{AbbottHrycak1990}. La recherche de tels patterns est une question importante dans plusieurs problématiques de sciences sociales \citep{Abbott1995}. Elle permet notamment de mettre en lumière les contraintes légales, économiques ou sociales qui encadrent la construction des parcours individuels. Comme le notent \citet{AbbottHrycak1990}, si les séquences types peuvent résulter de contrainte que l'on redécouvre, ces séquences typiques peuvent également agir sur la réalité en servant de modèles aux acteurs qui anticipent leur propre futur. Ces différentes possibilités d'interprétations font de la création de typologie un outil puissant.

Toutes les analyses en cluster produisent des résultats, quelle qu'en soit la pertinence \citep{Levine2000}. Il est donc nécessaire de discuter de sa qualité afin de préciser la portée des résultats et de ne pas faire de généralisation abusive. À notre sens, cette étape est trop souvent absente des analyses en cluster. Dans notre cas, cette qualité était faible, ce qui est courant en analyse de séquences. Avec une qualité plus élevée (c'est-à-dire $ASW>0.7$ par exemple), on pourra se montrer plus affirmatif, car la partition est vraisemblablement le reflet d'une structure forte identifiée dans les données.

Si l'outil est puissant, il est également risqué. En donnant un nom unique à chaque groupe, on tend à supprimer de l'analyse la diversité des situations rencontrées à l'intérieur de chaque groupe. À titre d'exemple, nous avions noté que la durée des périodes sans emploi varie sensiblement au sein du groupe que nous avons nommé \guil{Sans emploi} et que l'état se rencontre également dans d'autres groupes. Cette simplification fait sens si le but est de faire émerger les patterns récurrents dans une optique descriptive.

Toutefois, les typologies ne devraient pas être utilisées dans une démarche explicative. En effet, ceci revient à postuler l'existence de modèles clairement définis qui auraient effectivement généré les trajectoires et que l'on aurait identifiés grâce à l'analyse en clusters. Outre le fait que cette démarche peut conduire à des conclusions trompeuses si ces hypothèses ne se vérifient pas, ces hypothèses sont discutables d'un point de vue sociologique. Dans la lignée du paradigme des parcours de vie, on peut penser que les individus sont soumis à des influences et des contraintes diverses qui participent, chacune à leur manière, à la construction de la trajectoire (ou d'une partie de la trajectoire). On est ainsi bien loin de la recherche de modèles de trajectoires clairement définis.



\bibliography{manual}

\appendix

\section{Agréger les séquences identiques}
\label{sec_aggregate}
Les algorithmes que nous avons présentés prennent tous en compte la pondération des observations. Ceci permet de regrouper les observations identiques en leur donnant une pondération plus élevée. On peut ensuite effectuer l'analyse en clusters sur ces données regroupées, ce qui diminue considérablement temps de calcul et la mémoire utilisée\footnote{La quantité de mémoire nécessaire évolue de manière quadratique.}. On termine l'analyse en \guil{désagrégeant} les données afin de réintégrer la typologie dans les données initiales.

Ces différentes opérations sont réalisées aisément avec la fonction \code{wcAggregateCases} de la libraire \pkg{WeightedCluster}. Cette fonction identifie les cas identiques afin de les regrouper. Dans un deuxième temps, l'objet retourné permet également de réaliser l'opération inverse, c'est-à-dire de désagréger les données.

Reprenons l'exemple que nous avons utilisé depuis le début de ce manuel. Le code suivant permet d'identifier les séquences identiques. La fonction \code{wcAggregateCases} prend deux paramètres: un \code{data.frame} (ou une matrice) qui contient les cas à agréger ainsi qu'un vecteur de poids optionnel.

<<wcAggregateCases, echo=TRUE>>=
ac <- wcAggregateCases(mvad[, 17:86], weights=mvad$weight)
ac
@

L'objet renvoyé par la fonction \code{wcAggregateCases} (\code{ac} ici) donne quelques informations de base sur le regroupement. On peut ainsi noter que le jeu de donnée initial comporte 712 observations, mais seulement 490 séquences différentes. L'objet retourné contient trois éléments particulièrement intéressants.
\begin{itemize}
    \item \code{aggIndex}: index des objets uniques.
    \item \code{aggWeights}: Nombre de fois (éventuellement pondéré) que chaque objet unique de \code{aggIndex} apparaît dans les données.
    \item \code{disaggIndex}: index des objets initiaux dans la liste des objets uniques. Cette information permet de désagréger les données. On donnera plus tard un exemple d'utilisation.
\end{itemize}

À l'aide de ces informations, nous pouvons créer un objet \code{uniqueSeq} des séquences uniques pondérées par le nombre de fois qu'elles apparaissent dans les données. Dans le code suivant, \code{ac\$aggIndex} permet de sélectionner les séquences uniques et le vecteur \code{ac\$aggWeights} contient la pondération de chacune de ces séquences.

<<wcAggregateCases-seqdef, echo=TRUE>>=
uniqueSeq <- seqdef(mvad[ac$aggIndex, 17:86], alphabet = mvad.alphabet,
    states = mvad.scodes, labels = mvad.labels,  weights=ac$aggWeights)
@

À partir de là, nous pouvons calculer différentes solutions de clustering. Ici, nous calculons la matrice des distances avant d'utiliser cette information pour la fonction \code{wcKMedoids}. Comme précédemment, nous utilisons ici le vecteur \code{ac\$aggWeights} pour utiliser les pondérations.

<<wcAggregateCases-wckmedoids, echo=TRUE>>=
mvaddist2 <- seqdist(uniqueSeq, method="OM", indel=1.5, sm=subm.custom)
pamclust4ac <- wcKMedoids(mvaddist2, k=4, weights=ac$aggWeights)
@

Une fois le clustering réalisé, l'information contenue dans \code{ac\$disaggIndex} permet de revenir en arrière. On peut par exemple ajouter la typologie dans le \code{data.frame} original (non agrégé) à l'aide du code suivant. Le vecteur \code{pamclust4ac\$clustering} contient l'appartenance de chaque séquence unique aux clusters. En utilisant l'indice \code{ac\$disaggIndex}, on revient au jeu de donnée original, c'est-à-dire que l'on obtient l'appartenance de chaque séquence (non unique) aux clusters.

<<wcAggregateCases-wckmedoids-back, echo=TRUE>>=
mvad$acpam4 <- pamclust4ac$clustering[ac$disaggIndex]
@

Le tableau suivant donne la répartition des cas originaux entre la typologie que nous avions obtenue à partir des cas désagrégés (variable \code{pam4}) et celle obtenue à partir des données agrégées (variable \code{acpam4}). On remarquera que les deux solutions contiennent les mêmes cas, seuls les labels diffèrent.

<<wcAggregateCases-wckmedoids-table, echo=TRUE>>=
table(mvad$pam4, mvad$acpam4)
@

L'agrégation de cas identiques est une fonctionnalité très utile pour les grands jeux de données. Il est fréquent que l'on ne puisse pas calculer l'ensemble des distances pour cause de mémoire insuffisante. Dans de tels cas, l'utilisation de \code{wcAggregateCases} pourrait bien résoudre le problème.

%\appendix
%\section{Aggregating and disaggregating cases}
%
%The \pkg{WeightedCluster} library provides some utilities to aggregate identical cases with the \code{wcAggregateCases} function. It returns a \code{wcAggregateCases} object that may be used to aggregate or disaggregate cases. Lets take an example. We may aggregate our sequence object using \code{wcAggregateCases}. The sequence data are located in columns 17 to 86. We may also provide an optional case weights vector.
%
%<<wcAggregateCases, echo=TRUE>>=
%library(WeightedCluster)
%data(mvad)
%ac <- wcAggregateCases(mvad[, 17:86], weights=mvad$weight)
%ac
%@
%
%We may then use the returned \code{wcAggregateCases} object to regroup similar case. The index of unique object is stored in the \code{aggIndex} element and the number of time this unique object was found is stored in the \code{aggWeights}.
%
%<<seqdef-aggregated, echo=TRUE>>=
%mvadseq <- seqdef(mvad[ac$aggIndex, 17:86], weights=ac$aggWeights)
%@
%
%We may also use the returned object to go back and disaggregate previously aggregated data. Suppose we wish to compute state sequence complexity of our sequence and compute an anova with the father unemployment status (not used to aggregate data). We may compute the complexity.
%
%<<seqici-aggregated, echo=TRUE>>=
%complx <- seqici(mvadseq)
%@
%
%Add the complexity in our original data set.
%
%<<seqici-disaggregated, echo=TRUE>>=
%mvad$complx <- complx[ac$disaggIndex]
%@
%

\section{Notes sur les performances}
\label{annexe_optim}
Les algorithmes de partitionnement autour de centres mobiles disponibles dans la librairie \pkg{WeightedCluster} sont hautement optimisés. En interne, la librairie propose plusieurs variantes de l'algorithme PAM. Le choix entre ces variantes dépend du type de l'objet distance passé à la fonction ainsi que de l'argument méthode.

L'argument \code{diss} peut être une matrice de distance ou un objet \code{dist}. Dans le premier cas, chaque distance est enregistrée à double ce qui peut rapidement poser des problèmes de quantité de mémoire disponible, mais l'algorithme est généralement plus rapide (voir les comparaisons de performances ci-dessous). Si l'argument est de type \code{dist}, seul le triangle inférieur de la matrice de distance est stocké en mémoire, mais ce gain se fait au détriment de la rapidité de l'algorithme.

Contrairement à l'algorithme PAM disponible dans la librairie \pkg{cluster}, la matrice des distances n'est pas copiée en interne, il est donc possible de réaliser un clustering d'un nombre nettement plus important d'objets avant d'atteindre les limites mémoires de la machine.

L'argument \code{method} spécifie l'algorithme utilisé. Deux versions sont disponibles: la version originale de l'algorithme PAM et PAMonce. L'algorithme \guil{PAMonce} implémente les optimisations proposées par \citet{Reynolds2006} qui consiste à n'évaluer qu'une seule fois (plutôt que $n$ fois dans la version originale) le coût de la suppression d'un médoïde. Nous avons également inclus dans cet algorithme une deuxième optimisation qui consiste à ne pas évaluer le gain du remplacement d'un médoïde par un objet donné si la distance entre ceux-ci est nulle. Cette optimisation est cohérente avec la définition mathématique d'une mesure de distance selon laquelle deux objets sont identiques si, et seulement si, leur distance est nulle. Cette optimisation ne fait sens que dans la mesure où les objets à regrouper contiennent des doublons et surtout si la mesure de dissimilarité utilisée respecte cette condition. Notons que les mesures de dissimilarités la respectent généralement.

Afin de mesurer l'impact sur les performances de ces optimisations, nous avons conduit plusieurs simulations. Dans celles-ci, il s'agissait de regrouper en $k \in (2, 8, 16, 24, 32, 48, 64, 96, 128)$ groupes un ensemble de $n \in (200, 500, 1000, 2000)$ observations dont les coordonnées $x$ et $y$ ont été générées aléatoirement selon une distribution uniforme. La figure~\ref{perf-nbyk-plot-time} présente l'évolution du temps de calcul (sur une échelle logarithmique) en fonction des valeurs de $n$ et $k$. Quant à elle, la figure~\ref{perf-nbyk-plot} présente l'évolution du temps total relatif, c'est-à-dire divisé par le temps mis par l'algorithme le plus rapide pour cette solution, en fonction des mêmes paramètres.


<<perf-loadsimul, echo=FALSE, results="hide", message=FALSE>>=
load(file="randB.RData")
randB$nnn <- factor(randB$n, levels=c(200, 500, 1000, 2000), labels=c("n=200", "n=500", "n=1000", "n=2000"))
library(lattice)
@


\begin{figure}[htb]
\centering
\caption{Évolution du temps de calcul en fonction de $n$ et $k$}
\label{perf-nbyk-plot-time}
<<perf-nbyk-plot-time, fig.width=8, fig.height=3.5, echo=FALSE>>=
print(xyplot(ClusterTime~k|nnn, data=randB, groups=test, type="l", auto.key = list(points=F, lines=T,corner = c(0, 0.8), cex=.9, size=2), scales=list(y = list(log = TRUE)), ylab="Temps de calcul", xlab="Nombre de groupes"))
@
\end{figure}


\begin{figure}[htb]
\centering
\caption{Évolution du temps relatif en fonction de $n$ et $k$}
\label{perf-nbyk-plot}
<<perf-nbyk-plot, fig.width=8, fig.height=3.5, echo=FALSE>>=
print(xyplot(relative.tot~k|nnn, data=randB, groups=test, type="l", auto.key = list(points=F, lines=T,corner = c(0, 0.8), size=2, cex=.9), ylab="Temps relatif", xlab="Nombre de groupes"))
@
\end{figure}

Les solutions proposées par \pkg{WeightedCluster} sont plus rapides et les différences augmentent à mesure que $k$ et $n$ augmentent. L'utilisation d'une matrice de distances plutôt que l'objet \code{dist} permet d'accélérer les temps de calcul. Le gain de mémoire se fait donc bien au détriment du temps de calcul. Il en va de même pour les optimisations de \guil{PAMonce} qui apporte un gain significatif. Par rapport à la librairie \pkg{cluster}, les gains sont particulièrement important (15 fois environ) lorsque $n$ est plus grand que 1'000 et que $k$ est grand.


Rappelons encore que, si les données contiennent des cas identiques, les gains sont potentiellement encore plus importants, car ces cas peuvent être regroupés en utilisant \code{wcAggregateCases}. Cette opération réduit considérablement la mémoire nécessaire et le temps de calcul.

\section{Détails des mesures de qualité}
\label{annexe_clustqual}
\subsection{Average Silhouette Width (ASW)}
\label{annexe_asw}
Originellement proposée par \citet{Kaufman1990}, cet indice se base sur une notion de cohérence de l'assignation d'une observation à un groupe donné qui est mesurée en mettant en parallèle la distance moyenne pondérée, notée $a_i$, d'une observation $i$ aux autres membres de son groupe et la distance moyenne pondérée au groupe le plus proche, notée $b_i$.

Formellement, ces distances moyennes sont définies de la manière suivante. Soit $k$ le groupe de l'observation $i$, $W_k$ la somme des pondérations des observations appartenant au groupe $k$, $w_i$ le poids de l'observation $i$ et $\ell$ un des autres groupes, la silhouette d'une observation se calcule de la manière suivante.


\begin{eqnarray}
    a_i&=&\frac{1}{W_k-1}\sum_{j \in k} w_j d_{ij}\label{eq_clustqual_asw_ai}\\
    b_i&=&\min_{\ell} \frac{1}{W_\ell}\sum_{j \in \ell} w_j d_{ij}\\
    s_i&=&\frac{b_i - a_i}{\max(a_i, b_i)}\label{eq_clustqual_asw}
\end{eqnarray}

L'équation~\ref{eq_clustqual_asw_ai} suppose que l'unité des pondérations corresponde à une observation. Cette hypothèse découle de l'utilisation de $W_k-1$ dans l'équation~\ref{eq_clustqual_asw_ai}. Cette hypothèse ne pose aucun problème quand les pondérations résultent d'agrégation (voir l'annexe~\ref{sec_aggregate}) ou si les données ne sont pas pondérées. Lorsque certains $w_i$ ou si $W_k$ sont inférieurs à un cependant, les valeurs $a_i$ sont indéfinies et la silhouette ne peut donc être interprétée. Ces cas de figure sont fréquents quand lorsque les pondérations visent à corriger la représentativité de données d'enquêtes (comme c'est le cas dans notre base de donnée example). Pour pallier ce problème, nous proposons de remplacer $a_i$ par $aw_i$ qui se calcule de la manière suivante:

\begin{equation}
  aw_i=\frac{1}{W_k}\sum_{j \in k} w_j d_{ij}\label{eq_clustqual_asww_ai}\\
\end{equation}

la valeur $aw_i$ peut s'interpréter de deux manières. Elle correspond à la distance entre l'observation et son propre groupe, en considérant que toute les observations du groupes doivent être utilisée pour définir le groupe. Dans la formulation originale, l'observation est retirée du groupe pour calculer cette distance. La deuxième interprétation consiste à dire que l'unité des pondérations est aussi petite que possible, c'est-à-dire qu'elle tend vers zéro.

Dans les deux cas de figure, l'indice finalement utilisé correspond à la moyenne pondérée des silhouettes $s_i$. Cette valeur est retournée dans l'élément \code{stats}. La moyenne pondérée des silhouettes de chaque groupe est donnée dans l'élément \code{ASW} et mesure séparément la cohérence de chaque groupe.

\subsection{C index (HC)}

Développé par \citet{HubertLevin1976}, cet indice met en parallèle la partition obtenue et la meilleure partition que l'on aurait pu obtenir avec ce nombre de groupe et cette matrice de distance. L'indice varie entre 0 et 1, une petite valeur indiquant une bonne partition des données. Plus formellement, il est défini de la manière suivante. Soit $S$ la somme des distances intragroupes pondérée par le produit des poids de chaque observation, $W$ la somme des poids des distances intragroupes, $S_{min}$ la somme pondérée des $W$ plus petites distances, et $S_{max}$ la somme pondérée des $W$ plus grandes distances:

\begin{equation}
    C_{index}=\frac{S-S_{min}}{S_{max}-S_{min}}
\end{equation}


\subsection{\guil{Hubert's Gamma} (HG, HGSD) et \guil{Point Biserial Correlation} (PBC)}

Ces indices mesurent la capacité d'une partition à reproduire la matrice des distances en calculant l'association entre la distance originale $d$ et une deuxième matrice $d_{bin}$ prenant la valeur $0$ pour les observations classées dans la même partition et $1$ sinon. Pour utiliser les pondérations, nous utilisons $W^{mat}$ la matrice du produit des pondérations $W^{mat}_{ij}=w_i\cdot w_j$.

\citet{Hubert1985} proposent de mesurer cette association en utilisant le Gamma de Goodman et Kruskal (HG) ou le D de Somers (HGSD) afin de tenir compte des égalités dans la matrice des distances. \pkg{WeightedCluster} utilise les formules suivantes basées sur le fait que $d_{bin}$ ne prend que deux valeurs différentes. Soit $T$ le tableau croisé pondéré entre les valeurs de $d$ en ligne ($\ell$ lignes) et de $d_{bin}$ en deux colonnes (0 ou 1) calculé en utilisation les pondérations $W^{mat}$, le nombre de paires concordantes $C$, celui de paires discordantes $D$, celui des égalités sur $d$ $E$ sont définis de la manière suivante:

\begin{align}
  C&=\sum_{i=1}^\ell\sum_{i'=1}^{i-1}T_{i'0} &
  D&=\sum_{i=1}^\ell\sum_{i'=1}^{i-1}T_{i'1} &
  E&=\sum_{i=1}^\ell T_{i0}+T_{i1}\nonumber\\
  HG&=\frac{C-D}{C+D} & &&
  HGSD&=\frac{C-D}{C+D+E}\nonumber%\label{eq_hgsd}
\end{align}


 \citet{Hennig2010} proposent d'utiliser la corrélation de Pearson plutôt, solution également connue sous le nom de \guil{Point Biserial Correlation} (PBC équation \ref{eq_pbc}) \citep{MilliganCooper1985}. Soit $s_d$ et $s_{d_bin}$ l'écart-type pondéré par $W^{mat}$ de $d$ et $d_bin$ respectivement, $s_{d, d_{bin}}$ la covariance pondérée par $W^{mat}$ entre $d$ et $d_{bin}$, cette corrélation est calculée de la manière suivante:

\begin{equation}
  PBC=\frac{s_{d, d_{bin}}}{s_{d_{bin}}\cdot s_{d_{bin}}}\label{eq_pbc}
\end{equation}


\subsection{CH index (CH, CHsq, R2, R2sq)}

\citet{CalinskiHarabasz1974} proposent d'utiliser la statistique $F$ de l'analyse de variance en le calculant à partir des distances euclidiennes aux carrés. Sur les mêmes bases, on peut calculer un pseudo $R^2$, soit la part de la variabilité expliquée par une partition. \citet{StuderRitschardGabadinhoMuller2011SMR} proposent une extension aux données pondérées de ces mesures.

\section{Construction de la variable test}
\label{annexe_vartest}
La variable \code{test} est construite de façon à expliquer la variabilité à l'intérieur des clusters. Pour ce faire, on utilise un MDS pour données pondérées \citep{OksanenEtAl2012}.

<<mds-compute-annexe, echo=TRUE, results="hide">>=
library(vegan)
worsq <- wcmdscale(mvaddist, w=mvad$weight, k=2)
@

L'analyse en clusters explique principalement les différences sur le premier axe. On crée donc la variable test en fonction du deuxième axe. Afin que le test du khi-carré soit proche de zéro, il faut que les proportions de \guil{test} et de \guil{non-test} soient équivalentes dans chaque groupe. Pour satisfaire ces deux contraintes, on utilise la médiane du deuxième axe du MDS dans chaque groupe comme point de séparation.

<<mds-compute-annexe2, echo=TRUE, results="hide">>=
library(isotone)
mvad$test <- rep(-1, nrow(mvad))
for(clust in unique(pamclust4$clustering)){
    cond <- pamclust4$clustering == clust
    values <- worsq[cond, 2]
    mvad$test[cond] <- values> weighted.median(values, w=mvad$weight[cond])
}
mvad$test <- factor(mvad$test, levels=0:1, labels=c("non-test", "test"))
@


\end{document}
